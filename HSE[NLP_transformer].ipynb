{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b23f1dcc6fd44251a8f64b603c2ff7c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cf527d6b6144f80ba9170e48155d11b","IPY_MODEL_03d554f9a53347829bbf96d5b9983554","IPY_MODEL_88d7ffae804d485f9737a02d96eba9b0"],"layout":"IPY_MODEL_c2f277a1ceed46fd8148a39dff91845d"}},"0cf527d6b6144f80ba9170e48155d11b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_472bfa3066284e5ca693728c913b8dac","placeholder":"​","style":"IPY_MODEL_c857fecd50d94b38bcfbc17f2287b630","value":"Map (num_proc=8): 100%"}},"03d554f9a53347829bbf96d5b9983554":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_285e9d99dbbd4b66a1e4e23a1de1ba88","max":204045,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd969243f4a4462a8d21ef061d89dc07","value":204045}},"88d7ffae804d485f9737a02d96eba9b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a12fd34abd5d49609afa66dcf150aa86","placeholder":"​","style":"IPY_MODEL_e699971de7374f7688688913533292b5","value":" 204045/204045 [10:46&lt;00:00, 976.06 examples/s]"}},"c2f277a1ceed46fd8148a39dff91845d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"472bfa3066284e5ca693728c913b8dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c857fecd50d94b38bcfbc17f2287b630":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"285e9d99dbbd4b66a1e4e23a1de1ba88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd969243f4a4462a8d21ef061d89dc07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a12fd34abd5d49609afa66dcf150aa86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e699971de7374f7688688913533292b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b9464e0d5064f20b3dbcba5537cd4c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_797bb33112204340822cff251288b742","IPY_MODEL_7fc0aef944b34e19a8e6d4e2061abd17","IPY_MODEL_d81fa05c1b044a02a67e6ee7fac06e50"],"layout":"IPY_MODEL_9d05aa26861e46b3b2a26ff0dd3f3c0b"}},"797bb33112204340822cff251288b742":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5002fa17f2f04975aa23ff09123ed983","placeholder":"​","style":"IPY_MODEL_fe74a5438cb64b7fba17b7349e46760a","value":"Map (num_proc=8): 100%"}},"7fc0aef944b34e19a8e6d4e2061abd17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8184ae53e45b43da9f1375e5282ff165","max":11332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61beb9fc5a004aa3bc6faca0476da998","value":11332}},"d81fa05c1b044a02a67e6ee7fac06e50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58e9f745dd4c44068cf5861e77b7edda","placeholder":"​","style":"IPY_MODEL_3948e0a76c6d4703932e8cbb966d77ce","value":" 11332/11332 [00:33&lt;00:00, 606.54 examples/s]"}},"9d05aa26861e46b3b2a26ff0dd3f3c0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5002fa17f2f04975aa23ff09123ed983":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe74a5438cb64b7fba17b7349e46760a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8184ae53e45b43da9f1375e5282ff165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61beb9fc5a004aa3bc6faca0476da998":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58e9f745dd4c44068cf5861e77b7edda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3948e0a76c6d4703932e8cbb966d77ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"299b1bca832341d5b1f6a7292fb9e9a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22c6a99c5ae84fff90721d4613c97402","IPY_MODEL_c5e7abdd482640cca9e66d48ae020391","IPY_MODEL_70f8ad3a0dfd4e7cb9d70b4fc70ce9e2"],"layout":"IPY_MODEL_89a0faf05e7c40da9222c1cb9ed5ed1d"}},"22c6a99c5ae84fff90721d4613c97402":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31115807d9e34826be8ee022c72f11cb","placeholder":"​","style":"IPY_MODEL_3a00665ef9e84f639edfc8651c286277","value":"Map (num_proc=8): 100%"}},"c5e7abdd482640cca9e66d48ae020391":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd572752549d4af6a9cccb5ee231f354","max":11334,"min":0,"orientation":"horizontal","style":"IPY_MODEL_690cfb2dff6c447a9238f6ac0d9e0baa","value":11334}},"70f8ad3a0dfd4e7cb9d70b4fc70ce9e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3a6ba18d66544a893d918f82797402b","placeholder":"​","style":"IPY_MODEL_d798fcdffd024a07a06c3dfdb0396bd5","value":" 11334/11334 [00:33&lt;00:00, 679.09 examples/s]"}},"89a0faf05e7c40da9222c1cb9ed5ed1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31115807d9e34826be8ee022c72f11cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a00665ef9e84f639edfc8651c286277":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd572752549d4af6a9cccb5ee231f354":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"690cfb2dff6c447a9238f6ac0d9e0baa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3a6ba18d66544a893d918f82797402b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d798fcdffd024a07a06c3dfdb0396bd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8703943,"sourceType":"datasetVersion","datasetId":5220574},{"sourceId":8719890,"sourceType":"datasetVersion","datasetId":5232225},{"sourceId":8721119,"sourceType":"datasetVersion","datasetId":5233207}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Summarization\n\nYou task is to prepare a model that is suitable for text summarization.\nWe gonna use CNN_dailymail dataset","metadata":{"id":"WgGy-DX_J2tp"}},{"cell_type":"code","source":"!pip install datasets transformers evaluate rouge_score\nfrom datasets import load_dataset\nfrom evaluate import load\nimport sentencepiece as spm\n\nimport math\nimport torch\nfrom torch import nn\n\ndataset = load_dataset(\"abisee/cnn_dailymail\", '3.0.0')\nmetric = load(\"rouge\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wAWeRggJuUY","outputId":"7e6f9b14-43bf-4106-fd59-ba2cb09a8a9e","execution":{"iopub.status.busy":"2024-06-18T10:50:55.038411Z","iopub.execute_input":"2024-06-18T10:50:55.038760Z","iopub.status.idle":"2024-06-18T10:52:07.927504Z","shell.execute_reply.started":"2024-06-18T10:50:55.038731Z","shell.execute_reply":"2024-06-18T10:52:07.926529Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=3603fd9f4d402bceaaa0847f906a86dd7b71fcfb669f7aace609c711a9cdd960\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score, evaluate\nSuccessfully installed evaluate-0.4.2 rouge_score-0.1.2\n","output_type":"stream"},{"name":"stderr","text":"2024-06-18 10:51:17.479247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-18 10:51:17.479377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-18 10:51:17.587965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a04916099dc4fd683351135b962a010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fde8c9520fd44475bade71266c4816fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b32278368f3d458eb571b21a4a33a3e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30cd12e99d1d476db02e9d659bac83e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38473c637da44259d7a52cc8b837ea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2875a7a1b64558aa49960604d2f908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef1c2ee3638472b99128d1cd3d28264"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee90019c56c64dfba8c74917e9122061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf19c005bc6e49749d4440b17b796213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ee3823fb3c482fa6b7b4faa2dc5974"}},"metadata":{}}]},{"cell_type":"code","source":"all_texts = dataset['train']['article'] + dataset['validation']['article'] + dataset['test']['article']\nwith open('text.txt', 'w', encoding=\"utf-8\") as ptr:\n    for text in all_texts:\n        ptr.write(text + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:43:38.804444Z","iopub.execute_input":"2024-06-17T11:43:38.805229Z","iopub.status.idle":"2024-06-17T11:43:45.274052Z","shell.execute_reply.started":"2024-06-17T11:43:38.805186Z","shell.execute_reply":"2024-06-17T11:43:45.272726Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/working/text.txt'\n\n# let us train sentencepiece tokenizer\nspm.SentencePieceTrainer.train(\n    input=path,\n    model_prefix='cnn_dailymail',\n    vocab_size=32000,\n    model_type='bpe'\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T22:09:15.498386Z","iopub.execute_input":"2024-06-17T22:09:15.499999Z","iopub.status.idle":"2024-06-17T22:09:16.771502Z","shell.execute_reply.started":"2024-06-17T22:09:15.499961Z","shell.execute_reply":"2024-06-17T22:09:16.769868Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/text.txt\n  input_format: \n  model_prefix: cnn_dailymail\n  model_type: BPE\n  vocab_size: 32000\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/text.txt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/text.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# let us train sentencepiece tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcnn_dailymail\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbpe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m-> 1047\u001b[0m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOSError\u001b[0m: Not found: \"/kaggle/working/text.txt\": No such file or directory Error #2"],"ename":"OSError","evalue":"Not found: \"/kaggle/working/text.txt\": No such file or directory Error #2","output_type":"error"}]},{"cell_type":"code","source":"model_path = '/kaggle/input/previous/cnn_dailymail.model'","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:46.108761Z","iopub.execute_input":"2024-06-18T10:53:46.109846Z","iopub.status.idle":"2024-06-18T10:53:46.114070Z","shell.execute_reply.started":"2024-06-18T10:53:46.109797Z","shell.execute_reply":"2024-06-18T10:53:46.113080Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Загрузка модели\nsp = spm.SentencePieceProcessor(model_file=model_path)\nexample_text = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million fortune.\"\n# Токенизация\ntokens = sp.encode(example_text, out_type=str)\nprint('Original text: \\n' + example_text)\nprint('\\nText after tokenization: ')\nprint(tokens)\ndecoded_text = sp.decode(tokens)\nprint('\\nText after detokenization:')\nprint(decoded_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:46.313086Z","iopub.execute_input":"2024-06-18T10:53:46.313704Z","iopub.status.idle":"2024-06-18T10:53:46.368445Z","shell.execute_reply.started":"2024-06-18T10:53:46.313674Z","shell.execute_reply":"2024-06-18T10:53:46.367494Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Original text: \nHarry Potter star Daniel Radcliffe gains access to a reported £20 million fortune.\n\nText after tokenization: \n['▁Harry', '▁Potter', '▁star', '▁Daniel', '▁Radcliffe', '▁gains', '▁access', '▁to', '▁a', '▁reported', '▁£20', '▁million', '▁fortune', '.']\n\nText after detokenization:\nHarry Potter star Daniel Radcliffe gains access to a reported £20 million fortune.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\n# Загрузка токенизатора\ntokenizer = T5Tokenizer.from_pretrained(model_path, model_max_length=512)\n\n# Пример текста\nexample_text = \"Harry Potter star Daniel Radcliffe gains access to a reported £20 million fortune.\"\n\n# Токенизация\ninputs = tokenizer(example_text, return_tensors=\"pt\", max_length=512, truncation=True)\nprint(\"Tokenized input ids:\", inputs[\"input_ids\"])\n\n# Пример детокенизации\ndecoded_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\nprint(\"Decoded text:\", decoded_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:47.982880Z","iopub.execute_input":"2024-06-18T10:53:47.983219Z","iopub.status.idle":"2024-06-18T10:53:48.152568Z","shell.execute_reply.started":"2024-06-18T10:53:47.983194Z","shell.execute_reply":"2024-06-18T10:53:48.151636Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2004: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n  warnings.warn(\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"Tokenized input ids: tensor([[ 3781, 10741,   648,  3345, 23291, 16234,  2520,    27,     5,   814,\n          7940,   963,  9130, 31944,     2]])\nDecoded text: Harry Potter star Daniel Radcliffe gains access to a reported £20 million fortune.\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size\nprint(f\"Vocabulary size: {vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:48.271325Z","iopub.execute_input":"2024-06-18T10:53:48.271667Z","iopub.status.idle":"2024-06-18T10:53:48.276458Z","shell.execute_reply.started":"2024-06-18T10:53:48.271639Z","shell.execute_reply":"2024-06-18T10:53:48.275537Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Vocabulary size: 32000\n","output_type":"stream"}]},{"cell_type":"code","source":"display(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:49.390538Z","iopub.execute_input":"2024-06-18T10:53:49.391394Z","iopub.status.idle":"2024-06-18T10:53:49.398049Z","shell.execute_reply.started":"2024-06-18T10:53:49.391359Z","shell.execute_reply":"2024-06-18T10:53:49.396906Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"max_input_length = 1024\nmax_target_length = 128\n\ndef preprocess_function(examples):\n    inputs = [doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"highlights\"], max_length=max_target_length, truncation=True, padding='max_length')\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:49.768042Z","iopub.execute_input":"2024-06-18T10:53:49.768359Z","iopub.status.idle":"2024-06-18T10:53:49.774188Z","shell.execute_reply.started":"2024-06-18T10:53:49.768333Z","shell.execute_reply":"2024-06-18T10:53:49.773226Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True, num_proc=16)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T10:53:51.425738Z","iopub.execute_input":"2024-06-18T10:53:51.426647Z","iopub.status.idle":"2024-06-18T11:09:21.669063Z","shell.execute_reply.started":"2024-06-18T10:53:51.426612Z","shell.execute_reply":"2024-06-18T11:09:21.667456Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=16):   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ded61cfd24546a2ad17844a69dd1fa6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=16):   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce80d7f18f154611ae34592cf02c50dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=16):   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677abdee24e44bfdba622b892b569d53"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ndisplay(tokenized_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T11:16:02.480080Z","iopub.execute_input":"2024-06-18T11:16:02.480479Z","iopub.status.idle":"2024-06-18T11:16:02.489100Z","shell.execute_reply.started":"2024-06-18T11:16:02.480445Z","shell.execute_reply":"2024-06-18T11:16:02.488170Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Now you have to prepare your dataset\nUse any tokenization methods you'd like <br>\nIf you are using guides please cite your sourse","metadata":{"id":"ge-QYpOLTgqR"}},{"cell_type":"markdown","source":"## Declare  your model (6 pts)\nUse encoder-decoder transformer architecure of any size <br>\nYou can lookup for a T5 architecure\n\n\n\n","metadata":{"id":"F6Ke1FvoUC0c"}},{"cell_type":"code","source":"### CONFIG ###\nfrom torch.nn import functional as F\nimport numpy as np\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 128 # what is the maximum context length for predictions?\nblock_input = 1024\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nembedding_size = 300\nn_head = 8\nn_layer = 3\ndropout = 0.2\nvocab_size = 32101\n\n### ATTENTION ###\nclass Attention(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(embedding_size, head_size, bias=False)\n        self.query = nn.Linear(embedding_size, head_size, bias=False)\n        self.value = nn.Linear(embedding_size, head_size, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = (q @ k.transpose(-2,-1)) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\n### MultiHeadAttention ###\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Attention(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(head_size * n_head, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n    \n### CrossAttention ###\nclass CrossAttention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__() \n        self.key = nn.Linear(embedding_size, head_size, bias=False)\n        self.query = nn.Linear(embedding_size, head_size, bias=False)\n        self.value = nn.Linear(embedding_size, head_size, bias=False)\n        self.dropout = nn.Dropout(dropout) \n    \n    def forward(self, x, encoder_embs):\n        # we need to get encoder embeddings to get K and V matricies\n        B,T,C = x.shape\n        q = self.query(x) # (B,T,hs)\n        k = self.key(encoder_embs) # (B,T,hs)\n        v = self.value(encoder_embs) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n    \n### MultiHeadCrossAttention ###\nclass CrossMultiHeadAttention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([CrossAttention(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(head_size * n_head, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, encoder_embs):\n        out = torch.cat([h(x, encoder_embs) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n### MaskedAttention ###\nclass MaskedAttention(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n    def __init__(self, head_size):\n        super().__init__() \n        self.key = nn.Linear(embedding_size, head_size, bias=False)\n        self.query = nn.Linear(embedding_size, head_size, bias=False)\n        self.value = nn.Linear(embedding_size, head_size, bias=False)\n        self.tril = torch.tril(torch.ones(block_size, block_size)).to(device)\n        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MaskedMultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([MaskedAttention(head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(head_size * n_head, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n### LayerNorm ###\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features)).to(device)\n        self.b_2 = nn.Parameter(torch.zeros(features)).to(device)\n        self.eps = eps \n\n    def forward(self, x):\n        print(self.eps.device)\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * ((x - mean) / (std + eps)) + self.b_2\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embedding_size, 4 * embedding_size),\n            nn.ReLU(),\n            nn.Linear(4 * embedding_size, embedding_size),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n### EncoderBlock ###\n\nclass EncoderBlock(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = embedding_size // n_head\n        self.self_att = MultiHeadAttention(head_size)\n        self.ffwd = FeedFoward()\n        self.ln1 = nn.LayerNorm(embedding_size)\n        self.ln2 = nn.LayerNorm(embedding_size)\n\n    def forward(self, x):\n        x = x + self.self_att(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n### DecoderBlock ###\nclass DecoderBlock(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = embedding_size // n_head\n        self.masked_att = MaskedMultiHeadAttention(head_size)\n        self.cross_att = CrossMultiHeadAttention(head_size)\n        self.ffwd = FeedFoward()\n        self.ln1 = nn.LayerNorm(embedding_size)\n        self.ln2 = nn.LayerNorm(embedding_size)\n        self.ln3 = nn.LayerNorm(embedding_size)\n    def forward(self, x, encoder_emb):\n        x = x + self.masked_att(self.ln1(x))\n        x = x + self.cross_att(self.ln2(x), encoder_emb)\n        x = x + self.ffwd(self.ln3(x))\n        return x\n\n### MyModel  ###\nclass ChatGPT5(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_input = nn.Embedding(vocab_size, embedding_size).to(device)\n        self.position_embedding_input = nn.Embedding(block_input, embedding_size).to(device)\n        \n        self.token_embedding_output = nn.Embedding(vocab_size, embedding_size).to(device)\n        self.position_embedding_output = nn.Embedding(block_size, embedding_size).to(device)\n        \n        self.encoder_blocks = nn.Sequential(*[EncoderBlock().to(device) for _ in range(n_layer)])\n        \n        self.decoder_blocks = []\n        for i in range(n_layer):\n            self.decoder_blocks.append(DecoderBlock().to(device))\n         \n        self.lm_head = nn.Linear(embedding_size, vocab_size).to(device)\n        \n    def forward(self, input_seq, target_seq):\n        input_seq = input_seq.to(torch.long)\n        target_seq = target_seq.to(torch.long)\n        \n        input_emb = self.token_embedding_input(input_seq)\n        input_positions = self.position_embedding_input(torch.arange(0, block_input, dtype=torch.long, device=device))\n        \n        input_final = input_emb + input_positions\n        \n        target_emb = self.token_embedding_output(target_seq)\n        target_positions = self.position_embedding_output(torch.arange(0, block_size, dtype=torch.long, device=device))\n        \n        target_final = target_emb + target_positions\n        \n        encoder_output = self.encoder_blocks(input_final)\n        \n        temp_input = target_final\n        for i in range(n_layer):\n            temp_input = self.decoder_blocks[i](temp_input, encoder_output)\n        \n        decoder_output = temp_input\n        \n        out = self.lm_head(decoder_output)\n        return out\n        \n    def generate(self, input_sequence, tokenizer, need_to_tokenize=False, max_out_length=100):\n        input_sequence = (input_sequence.to(torch.long))\n        start_token, end_token = tokenizer.pad_token_id, tokenizer.eos_token_id  \n        if need_to_tokenize:\n            input_sequence = torch.Tensor(tokenizer.encode(input_sequence, max_length=256, padding=\"max_length\", return_tensors=\"pt\")).to(device) # Tokenizing string input\n            input_sequence = input_sequence.to(torch.long)\n        \n        input_positions = self.position_embedding_input(torch.arange(0, block_input, dtype=torch.long, device=device))\n        \n        if len(input_sequence.shape) < 3:\n            input_embeddings = self.token_embedding_input(input_sequence).unsqueeze(0)\n        else:\n            input_embeddings = self.token_embedding_input(input_sequence)\n        final_embeddings = input_positions + input_embeddings\n        encoder_output = self.encoder_blocks(final_embeddings)\n\n        generated_tokens = []\n        current_token = torch.tensor([[start_token]]).to(device)\n\n        for _ in range(max_out_length):\n            target_embeddings = self.token_embedding_output(current_token)\n            \n            temp_input = target_embeddings\n            for i in range(n_layer):\n                temp_input = self.decoder_blocks[i](temp_input, encoder_output)\n            decoder_output = temp_input \n            \n            output = self.lm_head(decoder_output)\n\n            next_token = torch.argmax(output, dim=-1)\n\n            generated_tokens.append(next_token.item())\n            current_token = next_token\n\n            if next_token.item() == end_token:\n                break\n\n        return generated_tokens","metadata":{"id":"3ytO4omUV4_f","execution":{"iopub.status.busy":"2024-06-18T14:50:53.692101Z","iopub.execute_input":"2024-06-18T14:50:53.692587Z","iopub.status.idle":"2024-06-18T14:50:53.745369Z","shell.execute_reply.started":"2024-06-18T14:50:53.692556Z","shell.execute_reply":"2024-06-18T14:50:53.744397Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"### Easy way\nIf you are struggling with some modules, you can use PyTorch layers:\n```\nnn.TransformerEncoderLayer   - 0.2 pts penalty\nnn.TransformerDecoderLayer   - 0.2 pts penalty\nnn.TransformerEncoder        - 0.5 pts penalty\nnn.TransformerDecoder        - 0.5 pts penalty\nnn.Transformer               - 1.0 pts penalty   \n\n```\n\n","metadata":{"id":"salVl_ROWf_W"}},{"cell_type":"markdown","source":"## Train your model (2 pts)\n\nYou are free to use any library, including the HuggingFace🤗","metadata":{"id":"mMQ61lOdY8qE"}},{"cell_type":"code","source":"model = ChatGPT5()\nmodel = model.to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-06-18T14:50:55.119642Z","iopub.execute_input":"2024-06-18T14:50:55.120384Z","iopub.status.idle":"2024-06-18T14:50:55.560909Z","shell.execute_reply.started":"2024-06-18T14:50:55.120339Z","shell.execute_reply":"2024-06-18T14:50:55.560018Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"ChatGPT5(\n  (token_embedding_input): Embedding(32101, 300)\n  (position_embedding_input): Embedding(1024, 300)\n  (token_embedding_output): Embedding(32101, 300)\n  (position_embedding_output): Embedding(128, 300)\n  (encoder_blocks): Sequential(\n    (0): EncoderBlock(\n      (self_att): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Attention(\n            (key): Linear(in_features=300, out_features=37, bias=False)\n            (query): Linear(in_features=300, out_features=37, bias=False)\n            (value): Linear(in_features=300, out_features=37, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=296, out_features=300, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=300, out_features=1200, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1200, out_features=300, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): EncoderBlock(\n      (self_att): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Attention(\n            (key): Linear(in_features=300, out_features=37, bias=False)\n            (query): Linear(in_features=300, out_features=37, bias=False)\n            (value): Linear(in_features=300, out_features=37, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=296, out_features=300, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=300, out_features=1200, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1200, out_features=300, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): EncoderBlock(\n      (self_att): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Attention(\n            (key): Linear(in_features=300, out_features=37, bias=False)\n            (query): Linear(in_features=300, out_features=37, bias=False)\n            (value): Linear(in_features=300, out_features=37, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=296, out_features=300, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=300, out_features=1200, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1200, out_features=300, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=300, out_features=32101, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T11:16:18.691202Z","iopub.execute_input":"2024-06-18T11:16:18.691980Z","iopub.status.idle":"2024-06-18T11:16:18.698012Z","shell.execute_reply.started":"2024-06-18T11:16:18.691943Z","shell.execute_reply":"2024-06-18T11:16:18.696670Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader,\n                learning_rate, n_epochs, lr_decay_factor, verbose=True,\n                loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id),\n                device='cuda' if torch.cuda.is_available() else 'cpu'):\n#     model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    current_lr = learning_rate\n    train_epoch_losses = []\n    val_epoch_losses = []\n\n    print(\"Training...\")\n    for epoch in range(1, n_epochs + 1):\n        current_lr *= lr_decay_factor\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = current_lr\n\n        # Training step\n        model.train()\n        train_epoch_loss = 0\n        for train_batch_idx, batch in enumerate(train_dataloader, start=1):\n            train_inputs = batch['input_ids'].to(device)\n            train_targets = batch['labels'].to(device)\n\n            if verbose: print(f\"\\rTrain batch: {train_batch_idx}/{len(train_dataloader)}, Avg batch loss: {train_epoch_loss/train_batch_idx:.6f}\", end='')\n\n            train_preds = model(train_inputs, train_targets)\n\n            train_targets_shifted = train_targets[:, 1:].contiguous().view(-1)\n            train_preds = train_preds[:, :-1, :].contiguous().view(-1, train_preds.size(-1))\n\n            train_batch_loss = loss_fn(train_preds, train_targets_shifted)\n            train_epoch_loss += train_batch_loss.item()\n\n            optimizer.zero_grad()\n            train_batch_loss.backward()\n            optimizer.step()\n\n        train_epoch_losses.append(train_epoch_loss/len(train_dataloader))\n\n        # Validation step\n        model.eval()\n        val_epoch_loss = 0\n        with torch.no_grad():\n            for val_batch_idx, batch in enumerate(val_dataloader, start=1):\n                val_inputs = batch['input_ids'].to(device)\n                val_targets = batch['labels'].to(device)\n\n                val_preds = model(val_inputs, val_targets)\n\n                val_targets_shifted = val_targets[:, 1:].contiguous().view(-1)\n                val_preds = val_preds[:, :-1, :].contiguous().view(-1, val_preds.size(-1))\n\n                val_batch_loss = loss_fn(val_preds, val_targets_shifted)\n                val_epoch_loss += val_batch_loss.item()\n\n        val_epoch_losses.append(val_epoch_loss/len(val_dataloader))\n\n        if verbose: print(f\"\\nEpoch: {epoch}, Train loss: {train_epoch_losses[-1]:.6f}, Val loss: {val_epoch_losses[-1]:.6f}, lr {current_lr:.6f}\\n\")\n\n    print(\"Training complete.\")\n    return train_epoch_losses, val_epoch_losses","metadata":{"id":"GZyRRJWaY8SY","execution":{"iopub.status.busy":"2024-06-18T11:16:20.562755Z","iopub.execute_input":"2024-06-18T11:16:20.563625Z","iopub.status.idle":"2024-06-18T11:16:20.577638Z","shell.execute_reply.started":"2024-06-18T11:16:20.563587Z","shell.execute_reply":"2024-06-18T11:16:20.576650Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, 1e-3, 2, 0.75)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T14:51:10.299483Z","iopub.execute_input":"2024-06-18T14:51:10.300171Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training...\nTrain batch: 17945/17945, Avg batch loss: 5.737606\nEpoch: 1, Train loss: 5.737900, Val loss: 5.239655, lr 0.000750\n\nTrain batch: 15983/17945, Avg batch loss: 5.040648","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Train batch: 17945/17945, Avg batch loss: 5.035937","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\ntorch.save(model.state_dict(), 'gpt.pth')\nFileLink(r'gpt.pth') ","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:23:30.682747Z","iopub.execute_input":"2024-06-18T18:23:30.683078Z","iopub.status.idle":"2024-06-18T18:23:31.072828Z","shell.execute_reply.started":"2024-06-18T18:23:30.683051Z","shell.execute_reply":"2024-06-18T18:23:31.071884Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/gpt.pth","text/html":"<a href='gpt.pth' target='_blank'>gpt.pth</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/new-version/gpt_last.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-06-18T14:15:51.591139Z","iopub.execute_input":"2024-06-18T14:15:51.591651Z","iopub.status.idle":"2024-06-18T14:15:51.722687Z","shell.execute_reply.started":"2024-06-18T14:15:51.591609Z","shell.execute_reply":"2024-06-18T14:15:51.721777Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import gc\ndef all_clean(model=None, train_dataloader=None, val_dataloader=None):\n    if model is not None:\n        del model\n    if train_dataloader is not None:\n        del train_dataloader\n    if val_dataloader is not None:\n        del val_dataloader\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"All clean\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:30:57.406123Z","iopub.execute_input":"2024-06-17T13:30:57.407197Z","iopub.status.idle":"2024-06-17T13:30:57.412791Z","shell.execute_reply.started":"2024-06-17T13:30:57.407163Z","shell.execute_reply":"2024-06-17T13:30:57.411659Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"all_clean()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:31:06.519404Z","iopub.execute_input":"2024-06-17T13:31:06.520040Z","iopub.status.idle":"2024-06-17T13:31:07.740309Z","shell.execute_reply.started":"2024-06-17T13:31:06.520006Z","shell.execute_reply":"2024-06-17T13:31:07.739273Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"All clean\n","output_type":"stream"}]},{"cell_type":"code","source":"for train_batch_idx, batch in enumerate(train_dataloader, start=1):\n    train_inputs = batch['input_ids'].to(device)\n    train_targets = batch['labels'].to(device)\n    print(f'Sample text:')\n    print(tokenizer.decode(train_inputs[0], skip_special_tokens=True))\n    print()\n    print(f'Expected:')\n    print(tokenizer.decode(train_targets[0], skip_special_tokens=True))\n    model.eval()\n    with torch.no_grad():\n        x = model.generate(train_inputs[0], tokenizer)\n        print()\n        print('Model answer:')\n        print(tokenizer.decode(x, skip_special_tokens=True))\n    break ","metadata":{"execution":{"iopub.status.busy":"2024-06-18T14:47:43.981971Z","iopub.execute_input":"2024-06-18T14:47:43.982673Z","iopub.status.idle":"2024-06-18T14:47:44.862620Z","shell.execute_reply.started":"2024-06-18T14:47:43.982642Z","shell.execute_reply":"2024-06-18T14:47:44.861617Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Sample text:\nBy. Associated Press. Stacey Dean Rambold was only given a one month prison sentence for his rape against a 14-year-old victim he said 'looked older than her age' A former Billings, Montana teacher convicted of rape wrongly blamed his 14-year-old victim when he argued that she bore responsibility in the case, an attorney for the state said on Thursday in a court filing. The office of Attorney General Tim Fox is appealing Stacey Dean Rambold's one-month prison sentence to the Montana Supreme Court. The state says he should have served a mandatory minimum of four years after pleading guilty to sexual assault without consent. Rambold, 54, is fighting the appeal. In a brief submitted to justices Thursday, Assistant Attorney General Tammy Plubell chastised Rambold for suggesting the victim should share in the blame. That assertion from Rambold was contained in arguments submitted by his attorneys. Victim Cherice Moralez killed herself while the case against Rambold was pending. Rambold was 47 at the time of his crime. Under state law, children younger than 16 cannot consent to sexual intercourse. 'Rambold continues to perpetuate the myth that a 14-year-old girl should bear responsibility for her sexual victimization by a 47-year-old teacher,' Plubell wrote. 'The law, though, rightly defines her as blameless.' Rambold's attorneys have argued that the original sentence from state District Judge G. Todd Baugh was appropriate. In a brief submitted to the state's high court last month, they urged justices to reject the appeal. Citing Moralez's statements in interviews with law enforcement recorded before her death, Rambold attorneys Jay Lansing and Nancy Schwartz said there was 'no rational basis' to conclude a 14-year-old could bear responsibility only as the offender in a rape case. A 14-year-old victim can bear responsibility, too, the attorneys suggested. The interviews are under court seal and have not been made public. Lansing and Schwartz did not immediately respond to requests for comment. They have turned down previous interview requests. Rambold said that his 14-year-old victim Cherice Moralez (pictured here) was responsible for his actions because she appeared older than her age. The victim Cherice Moralez killed herself while the case against Rambold was pending. Moralez was a freshman at Billings High School when she was raped. Rambold was her business teacher. She committed suicide in 2010, an act her mother said was driven largely by the rape. The girl's death took away the prosecution's main witness, and Rambold initially avoided prison under a deferred-prosecution agreement that included sex-offender treatment. He violated the agreement by having unauthorized visits with relatives' children and having a sexual relationship with an adult woman, according to court documents. That re-opened the case and led to the August sentence from Baugh of 15 years in prison with all but one month suspended. Rambold was released in September. Baugh unsuccessfully sought to impose a longer term after coming under criticism for saying during the sentencing hearing that the victim 'appeared older than her chronological age.' Montana's Judicial Standards Commission has asked the Supreme Court to censure Baugh for imposing an unlawful sentence and blaming the child victim. The panel investigated the case after receiving hundreds of complaints about the judge. Freed: Stacey Rambold, seen here in August 2013, was freed from a Montana jail after only spending a month in prison for raping his 14-year-old student, who has since killed herself.\n\nExpected:\nAttorney General Tim Fox is appealing Stacey Dean Rambold's one-month prison sentence to the Montana Supreme Court. The state says he should have served a mandatory minimum of four years after pleading guilty to sexual assault without consent. Victim Cherice Moralez killed herself in 2010 while the case against Rambold was pending. Moralez was a freshman at Billings High School when she was raped and Rambold, 47 at the time, was her business teacher. 'Rambold continues to perpetuate the myth. that a 14-year-old girl should bear responsibility for her sexual. victimization by a 47-year-old teacher,' said Assistant\n\nModel answer:\n, rape victim's mother, 17, 2012 rape victim sexually assaulting his sentencing hearing. Victim sexually assaulting his mother Donna Brazile rape victim. Charged with rape victim sexually assaulting her mother, rape victim. Judge ruled she was convicted rapist Ched Evans, rape victim told him guilty verdict Thursday morning. 14-year-old classmate. Judge ruled her prison sentence suspended sentence.\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:26:16.642855Z","iopub.execute_input":"2024-06-18T18:26:16.643252Z","iopub.status.idle":"2024-06-18T18:26:16.648474Z","shell.execute_reply.started":"2024-06-18T18:26:16.643220Z","shell.execute_reply":"2024-06-18T18:26:16.647442Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"for train_batch_idx, batch in enumerate(test_dataloader, start=1):\n    train_inputs = batch['input_ids'].to(device)\n    train_targets = batch['labels'].to(device)\n    print(f'Sample text:')\n    print(tokenizer.decode(train_inputs[0], skip_special_tokens=True))\n    print()\n    print(f'Expected:')\n    print(tokenizer.decode(train_targets[0], skip_special_tokens=True))\n    model.eval()\n    with torch.no_grad():\n        x = model.generate(train_inputs[0], tokenizer)\n        print()\n        print('Model answer:')\n        print(tokenizer.decode(x, skip_special_tokens=True))\n    break ","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:39:23.877831Z","iopub.execute_input":"2024-06-18T18:39:23.878211Z","iopub.status.idle":"2024-06-18T18:39:24.190748Z","shell.execute_reply.started":"2024-06-18T18:39:23.878181Z","shell.execute_reply":"2024-06-18T18:39:24.189819Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"Sample text:\n(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n\nExpected:\nMembership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June. Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis.\n\nModel answer:\nRangel says Hamas.\n","output_type":"stream"}]},{"cell_type":"code","source":"loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\nts_epoch_losses = []\nmodel.eval()\nts_epoch_loss = 0\nwith torch.no_grad():\n    for ts_batch_idx, batch in enumerate(test_dataloader, start=1):\n        ts_inputs = batch['input_ids'].to(device)\n        ts_targets = batch['labels'].to(device)\n\n        ts_preds = model(ts_inputs, ts_targets)\n\n        ts_targets_shifted = ts_targets[:, 1:].contiguous().view(-1)\n        ts_preds = ts_preds[:, :-1, :].contiguous().view(-1, ts_preds.size(-1))\n\n        ts_batch_loss = loss_fn(ts_preds, ts_targets_shifted)\n        ts_epoch_loss += ts_batch_loss.item()\n\nts_epoch_losses.append(ts_epoch_loss/len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:32:48.319898Z","iopub.execute_input":"2024-06-18T18:32:48.320558Z","iopub.status.idle":"2024-06-18T18:33:52.259157Z","shell.execute_reply.started":"2024-06-18T18:32:48.320529Z","shell.execute_reply":"2024-06-18T18:33:52.258321Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"ts_epoch_losses","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:38:58.001004Z","iopub.execute_input":"2024-06-18T18:38:58.001737Z","iopub.status.idle":"2024-06-18T18:38:58.007377Z","shell.execute_reply.started":"2024-06-18T18:38:58.001705Z","shell.execute_reply":"2024-06-18T18:38:58.006458Z"},"trusted":true},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"[5.071946619283175]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine tune a model from HuggingFace (2 pts)\n\nUse a [guide](https://huggingface.co/docs/transformers/tasks/summarization) from HuggingFace🤗","metadata":{"id":"MRYsARUhYJ5n"}},{"cell_type":"code","source":"!pip -q install accelerate -U\n!pip -q install transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:39:44.333003Z","iopub.execute_input":"2024-06-18T18:39:44.333631Z","iopub.status.idle":"2024-06-18T18:40:10.291149Z","shell.execute_reply.started":"2024-06-18T18:39:44.333599Z","shell.execute_reply":"2024-06-18T18:40:10.289900Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\npretrained_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:43:53.072216Z","iopub.execute_input":"2024-06-18T18:43:53.073081Z","iopub.status.idle":"2024-06-18T18:44:01.310561Z","shell.execute_reply.started":"2024-06-18T18:43:53.073043Z","shell.execute_reply":"2024-06-18T18:44:01.309638Z"},"trusted":true},"execution_count":118,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5872e6c451491fa4223206e116148f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27868bf3a9544bb919bf6241d1c5143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"306e7c9f728243179bd1556d5275665b"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"id":"xkLvGFFyZKwJ","execution":{"iopub.status.busy":"2024-06-18T18:44:04.961434Z","iopub.execute_input":"2024-06-18T18:44:04.962322Z","iopub.status.idle":"2024-06-18T18:44:04.968972Z","shell.execute_reply.started":"2024-06-18T18:44:04.962289Z","shell.execute_reply":"2024-06-18T18:44:04.967912Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=64, shuffle=True)\nval_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:44:05.532242Z","iopub.execute_input":"2024-06-18T18:44:05.533105Z","iopub.status.idle":"2024-06-18T18:44:05.815971Z","shell.execute_reply.started":"2024-06-18T18:44:05.533071Z","shell.execute_reply":"2024-06-18T18:44:05.814986Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"why_do_i_need_a_dir_for_that\",\n    eval_strategy=\"epoch\",\n    learning_rate=1e-3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.1,\n    save_total_limit=3,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=pretrained_model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer, \n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-18T18:44:06.742506Z","iopub.execute_input":"2024-06-18T18:44:06.743208Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ····\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 4\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare your models (2 pts)\n\nUse rouge metric. See the example above. <br>\n\n\nFeed your own texts to both model and see the result\n\n\nAdd a conclusion","metadata":{"id":"gDrv2mxyZNMx"}},{"cell_type":"code","source":"### METRICS EVALUATION ###","metadata":{"id":"8VGN1HtzZoc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### My conclusion:","metadata":{"id":"G4gvzA8AZscX"}}]}